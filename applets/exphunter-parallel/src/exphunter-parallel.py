#!/usr/bin/env python
# exphunter-parallel 0.0.1
# Generated by dx-app-wizard.
#
# Parallelized execution pattern: Your app will generate multiple jobs
# to perform some computation in parallel, followed by a final
# "postprocess" stage that will perform any additional computations as
# necessary.
#
# See https://documentation.dnanexus.com/developer for documentation and
# tutorials on how to modify this file.
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/

import os
import dxpy
import subprocess
from pathlib import Path

@dxpy.entry_point("postprocess")
def postprocess(suboutputs):
    '''
    Postprocessing step to gather exphunters JSONs and combine into a single simplified output
    Currently a placeholder until there is more clarity on required output for association testing
    '''

    #for x in suboutputs:   
    #    dxpy.download_dxfile(dxpy.DXFile(x).get_id(), 'current.json')
    #    data = pd.read_json()
    return {'outvcfs': suboutputs}
    

@dxpy.entry_point("process")
def process(cram, crai, buildfile, varcatalog, mode, sex):
    ''' 
    Primary subprocess to run exphunter on a single CRAM
    Code duplicated from basic exphunter applet 
    '''
    
    # Download files
    dxpy.download_dxfile(cram, 'readfile.cram')
    dxpy.download_dxfile(crai, 'readfile.cram.crai')
    dxpy.download_dxfile(buildfile, 'refbuild.fa')
    dxpy.download_dxfile(varcatalog, 'varcatalog.json')

    # Parse EID and varcatalog name to use in naming output files
    eid = dxpy.DXFile(cram).name.split('_')[0]
    catalogname = Path(dxpy.DXFile(varcatalog).name).stem.split('_',2)[2]
    outprefix = f'{eid}_{catalogname}'

    # Run Expansion Hunter as subprocess (CRAI is not passed but must be available on instance)
    cmd = f"""ExpansionHunter --reads readfile.cram --reference refbuild.fa --variant-catalog varcatalog.json --output-prefix {outprefix} --analysis-mode {mode} --sex {sex} --threads 16"""
    subprocess.check_call(cmd, shell=True)

    # Upload JSON and VCF and return metadata as output 
    outjson = dxpy.upload_local_file(f"{outprefix}.json")
    outvcf = dxpy.upload_local_file(f"{outprefix}.vcf")
    return {'outjson': dxpy.dxlink(outjson), 'outvcf': dxpy.dxlink(outvcf)}


@dxpy.entry_point("main")
def main(cramlist, buildfile, varcatalog, mode, sex):
    '''
    Top-level function orchestrating parallelisation and postprocessing
    '''

    # Download input list of CRAMs (from provide DX File ID)
    dxpy.download_dxfile(cramlist, "cramlist.txt")

    # Process cramlist to a list of tuples with paired CRAM-CRAI DXFile objects to pass to jobs
    with open('cramlist.txt', 'r') as f:
        filelist = [x.strip('\n') for x in f.readlines()]
    filepairs = list()
    for x in filelist:
        crai_name = f'{dxpy.DXFile(x).name}.crai'
        crai_id = dxpy.find_one_data_object(name=crai_name)['id']
        filepairs.append((x, crai_id))
    print(filepairs)
    
    # Spawn new exphunter jobs for each filepair and track submissions in a list of DX job objects
    subjobs = list()
    for x in filepairs:
        subjob_input = {
            'cram': x[0], 
            'crai': x[1],
            'buildfile': buildfile, 
            'varcatalog': varcatalog,
            'mode': mode,
            'sex': sex
        }
        subjobs.append(dxpy.new_dxjob(subjob_input, "process"))

    # Spawn postprocessing job taking outputs of subjobs as input and capture DX job object 
    postprocess_job = dxpy.new_dxjob({"suboutputs": [subjob.get_output_ref("outvcf") for subjob in subjobs] }, "postprocess")

    return {"outvcfs": postprocess_job.get_output_ref("outvcfs")}

# Run everything specified above
dxpy.run()
