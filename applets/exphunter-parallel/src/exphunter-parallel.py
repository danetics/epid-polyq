#!/usr/bin/env python
# exphunter-parallel 0.0.1
# Generated by dx-app-wizard.
#
# Parallelized execution pattern: Your app will generate multiple jobs
# to perform some computation in parallel, followed by a final
# "postprocess" stage that will perform any additional computations as
# necessary.
#
# See https://documentation.dnanexus.com/developer for documentation and
# tutorials on how to modify this file.
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/

import os
import dxpy
import subprocess
from pathlib import Path

@dxpy.entry_point("postprocess")
def postprocess(vcflist, jsonlist):
    '''
    Postprocessing step to gather exphunters JSONs and combine into a single simplified output
    Currently a placeholder until there is more clarity on required output for association testing
    Simply returns the VCF outputs of each job 
    '''

    #for x in suboutputs:   
    #    dxpy.download_dxfile(dxpy.DXFile(x).get_id(), 'current.json')
    #    data = pd.read_json()
    return {'vcflist': vcflist, 'jsonlist': jsonlist}
    

@dxpy.entry_point("process")
def process(cram, buildfile, varcatalog, mode, sex):
    ''' 
    Primary subprocess to run exphunter on a single CRAM
    Code duplicated from basic exphunter applet 
    '''
    
    # Identify CRAI file corresponding to input CRAM
    crai_name = f'{dxpy.DXFile(cram).name}.crai'
    crai = dxpy.find_one_data_object(name=crai_name)['id']
    
    # Download files
    dxpy.download_dxfile(cram, 'readfile.cram')
    dxpy.download_dxfile(crai, 'readfile.cram.crai')
    dxpy.download_dxfile(buildfile, 'refbuild.fa')
    dxpy.download_dxfile(varcatalog, 'varcatalog.json')

    # Parse EID and varcatalog name to use in naming output files
    eid = dxpy.DXFile(cram).name.split('_')[0]
    catalogname = Path(dxpy.DXFile(varcatalog).name).stem.split('_',2)[2]
    outprefix = f'{eid}_{catalogname}'

    # Run Expansion Hunter as subprocess (CRAI is not passed but must be available on instance)
    cmd = f"""ExpansionHunter --reads readfile.cram --reference refbuild.fa --variant-catalog varcatalog.json --output-prefix {outprefix} --analysis-mode {mode} --sex {sex} --threads 16"""
    subprocess.check_call(cmd, shell=True)

    # Upload JSON and VCF and return metadata as output 
    outjson = dxpy.upload_local_file(f"{outprefix}.json")
    outvcf = dxpy.upload_local_file(f"{outprefix}.vcf")
    return {'outjson': dxpy.dxlink(outjson), 'outvcf': dxpy.dxlink(outvcf)}


@dxpy.entry_point("main")
def main(cramlist, buildfile, varcatalog, mode, sex):
    '''
    Top-level function orchestrating parallelisation and postprocessing
    '''

    # Download input list of CRAMs (from provide DX File ID)
    dxpy.download_dxfile(cramlist, "cramlist.txt")

    # Process cramlist to a list of file IDs
    with open('cramlist.txt', 'r') as f:
        filelist = [x.strip('\n') for x in f.readlines()]
    print(f'Cramlist contains {len(filelist)} DX IDs')
    
    # Spawn new exphunter jobs for each CRAM and track submissions in a list of DX job objects
    subjobs = list()
    for cram in filelist:
        subjob_input = {
            'cram': dxpy.DXFile(cram).get_id(),
            'buildfile': buildfile, 
            'varcatalog': varcatalog,
            'mode': mode,
            'sex': sex
        }
        subjobs.append(dxpy.new_dxjob(subjob_input, "process"))

    # Spawn postprocessing job taking outputs of subjobs as input and capture DX job object 
    inputs = {
        "vcflist": [x.get_output_ref('outvcf') for x in subjobs],
        "jsonlist": [x.get_output_ref('outjson') for x in subjobs]
    }
    postprocess_job = dxpy.new_dxjob(inputs, "postprocess")

    # Return all subjob files
    outputs = {
        "vcflist": postprocess_job.get_output_ref('vcflist'),
        "jsonlist": postprocess_job.get_output_ref('jsonlist')
    }
    return outputs

# Run everything specified above
dxpy.run()
